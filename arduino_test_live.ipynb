{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting fruits in real-time. Press 'q' to quit...\n",
      "Coordinates: (288, 37)\n",
      "Coordinates: (1245, 69)\n",
      "Coordinates: (1196, 756)\n",
      "Coordinates: (280, 748)\n",
      "\n",
      "0: 448x640 1 banana, 1 cell phone, 1655.4ms\n",
      "Speed: 0.0ms preprocess, 1655.4ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Coordinates: (286, 38)\n",
      "Coordinates: (1244, 71)\n",
      "Coordinates: (1196, 780)\n",
      "Coordinates: (278, 743)\n",
      "\n",
      "0: 448x640 1 banana, 1 cell phone, 1 refrigerator, 1942.2ms\n",
      "Speed: 12.3ms preprocess, 1942.2ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 banana, 1 refrigerator, 1867.5ms\n",
      "Speed: 11.4ms preprocess, 1867.5ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 banana, 1 refrigerator, 1701.1ms\n",
      "Speed: 10.0ms preprocess, 1701.1ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m fruit_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChoose index following [0, 1, 2] = [banana, apple, orange]: \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    102\u001b[0m target_fruit \u001b[38;5;241m=\u001b[39m fruits_list[fruit_index]  \u001b[38;5;66;03m# Specify the fruit to detect\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m \u001b[43mdetect_fruit_real_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_fruit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target_fruit_positions:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target fruit detected. Exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m, in \u001b[0;36mdetect_fruit_real_time\u001b[1;34m(target_fruit, camera_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m, img_resized)\n\u001b[0;32m     46\u001b[0m cv2\u001b[38;5;241m.\u001b[39msetMouseCallback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m, get_coordinates)  \u001b[38;5;66;03m# Set the callback function\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# YOLO object detection\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import serial\n",
    "import time\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load YOLOv8 large model\n",
    "model = YOLO('yolov8l.pt')\n",
    "model.to('cpu')\n",
    "\n",
    "def detect_fruit_and_save_image(target_fruit='apple', output_path='output_image.jpg', camera_index=0):\n",
    "    # Access the live camera feed\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Press 's' to capture the image when ready...\")\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "\n",
    "        # Resize the frame for YOLO detection (optional)\n",
    "        img_resized = cv2.resize(img, (1300, 900))\n",
    "\n",
    "        # Display the live feed\n",
    "        cv2.imshow('Live Camera Feed', img_resized)\n",
    "\n",
    "        # Wait for keypress\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):  # 's' to save the image\n",
    "            print(\"Image captured!\")\n",
    "            break\n",
    "        elif key == ord('q'):  # 'q' to quit without saving\n",
    "            print(\"Quit without capturing.\")\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            return None\n",
    "\n",
    "    # Close the camera feed\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # YOLO object detection\n",
    "    results = model(img_resized)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = model.names\n",
    "\n",
    "    # Find the class ID for the target fruit\n",
    "    target_fruit_class_id = None\n",
    "    for class_id, name in class_names.items():\n",
    "        if name == target_fruit:\n",
    "            target_fruit_class_id = class_id\n",
    "            break\n",
    "\n",
    "    if target_fruit_class_id is None:\n",
    "        print(f\"Error: The target fruit '{target_fruit}' is not in the YOLO dataset.\")\n",
    "        return None\n",
    "\n",
    "    target_fruit_positions = []\n",
    "\n",
    "    # Loop over detections\n",
    "    for detection in results[0].boxes:\n",
    "        class_id = int(detection.cls)  # Class ID of the detected object\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = map(int, detection.xyxy[0])\n",
    "        # Calculate the center of the bounding box\n",
    "        x_center = (xmin + xmax) // 2\n",
    "        y_center = (ymin + ymax) // 2\n",
    "\n",
    "        # Draw a red dot if the detected fruit is the target fruit, else blue dot\n",
    "        if class_id == target_fruit_class_id:\n",
    "            cv2.circle(img_resized, (x_center, y_center), 10, (0, 0, 255), -1)  # Red dot for target fruit\n",
    "            target_fruit_positions.append((x_center, y_center))\n",
    "        elif class_id in [46, 47, 49, 50, 51]:  # IDs corresponding to fruits (banana, apple, orange, etc.)\n",
    "            cv2.circle(img_resized, (x_center, y_center), 10, (255, 0, 0), -1)  # Blue dot for other fruits\n",
    "\n",
    "    # Save the detection image\n",
    "    cv2.imwrite(output_path, img_resized)\n",
    "\n",
    "    if target_fruit_positions:\n",
    "        print(f\"Detected {target_fruit} at positions: {target_fruit_positions}\")\n",
    "    else:\n",
    "        print(f\"No {target_fruit} detected in the image.\")\n",
    "\n",
    "    return target_fruit_positions\n",
    "\n",
    "# Live camera setup, replace with your camera index\n",
    "output_image = 'box_with_fruits_and_edges_marked.jpg'  # Path to save the image with marked dots\n",
    "\n",
    "fruits_list = ['banana', 'apple', 'orange']\n",
    "fruit_index = int(input('Choose index following [0, 1, 2] = [banana, apple, orange]: '))\n",
    "target_fruit = fruits_list[fruit_index]  # Specify the fruit to mark with red dots\n",
    "\n",
    "target_fruit_positions = detect_fruit_and_save_image(target_fruit, output_image)\n",
    "\n",
    "if not target_fruit_positions:\n",
    "    print(\"No target fruit detected. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Placeholder box coordinates for calculating position (replace with actual box coordinates)\n",
    "coordinates_of_box = [(100, 100), (1200, 100), (1200, 800), (100, 800)]\n",
    "\n",
    "x_zero, y_zero = coordinates_of_box[2] #bottom-right corner of box\n",
    "x_fruit, y_fruit = target_fruit_positions[0]\n",
    "\n",
    "box_size_x_mm = 282 #mm (measured IRL)\n",
    "box_size_y_mm = 240 #mm (measured IRL)\n",
    "box_size_diag_mm = (box_size_x_mm**2 + box_size_y_mm**2)**0.5 #mm (measured IRL)\n",
    "\n",
    "# Calculate distances from picture in x\n",
    "box_size_x_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[1])) #mm (width in pixels from picture)\n",
    "box_size_x_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[2])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "\n",
    "# Calculate distances from picture in y\n",
    "box_size_y_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "box_size_y_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[1])-np.array(coordinates_of_box[2])) #mm (width in pixels from picture)\n",
    "\n",
    "# Calculate distances from picture in diagonals\n",
    "box_size_diag_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[2])) #mm (width in pixels from picture)\n",
    "box_size_diag_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[1])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "\n",
    "# Get mean distances for x and y\n",
    "box_size_x_pixels_mean = (box_size_x_pixels_1 + box_size_x_pixels_2) / 2\n",
    "box_size_y_pixels_mean = (box_size_y_pixels_1 + box_size_y_pixels_2) / 2\n",
    "box_size_diag_pixels_mean = (box_size_diag_pixels_1 + box_size_diag_pixels_2) / 2\n",
    "\n",
    "# Get mm per pixel for x and y\n",
    "mm_per_pixel_x = box_size_x_mm / box_size_x_pixels_mean # mm / pixel\n",
    "mm_per_pixel_y = box_size_y_mm / box_size_y_pixels_mean # mm / pixel\n",
    "mm_per_pixel_diag = box_size_diag_mm / box_size_diag_pixels_mean # mm / pixel\n",
    "\n",
    "# Get overall mean mm per pixel\n",
    "mm_per_pixel = (mm_per_pixel_x + mm_per_pixel_y + mm_per_pixel_diag) / 3\n",
    "    \n",
    "relative_x_fruit_pixels = abs(x_fruit - x_zero)\n",
    "relative_y_fruit_pixels = abs(y_fruit - y_zero)\n",
    "\n",
    "relative_x_fruit_mm = relative_x_fruit_pixels*mm_per_pixel\n",
    "relative_y_fruit_mm = relative_y_fruit_pixels*mm_per_pixel\n",
    "    \n",
    "print(f'\\nPixels from {target_fruit} to bottom-right corner:')\n",
    "print('x (pixels):', relative_x_fruit_pixels)\n",
    "print('x (mm):', relative_x_fruit_mm)\n",
    "print('y (pixles):', relative_y_fruit_pixels)\n",
    "print('y (mm):', relative_y_fruit_mm)\n",
    "\n",
    "# Distances from bottom-right corner of box to the base system of Hubert\n",
    "box_bottom_right_corner_to_base_system_x = 60 #mm\n",
    "box_bottom_right_corner_to_base_system_y = -120  #mm\n",
    "box_bottom_right_corner_to_base_system_z = 94 #mm\n",
    "\n",
    "if fruit_index == 0: #banana\n",
    "    relative_z_fruit_mm = 70 #start with set value to begin with\n",
    "else: #apple or orange\n",
    "    relative_z_fruit_mm = 70\n",
    "\n",
    "fruit_position_base_system_x = box_bottom_right_corner_to_base_system_x + relative_y_fruit_mm\n",
    "fruit_position_base_system_y = box_bottom_right_corner_to_base_system_y + relative_x_fruit_mm\n",
    "fruit_position_base_system_z = box_bottom_right_corner_to_base_system_z + relative_z_fruit_mm\n",
    "\n",
    "fruit_position_base_system = [fruit_position_base_system_x, fruit_position_base_system_y, fruit_position_base_system_z]\n",
    "\n",
    "print(\"\\nFruit pos base system (mm): \", fruit_position_base_system)\n",
    "\n",
    "class InverseKinematicsNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InverseKinematicsNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)  # Input layer (x, y, z) -> 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc3 = nn.Linear(128, 256)  # Hidden layer -> 128 neurons\n",
    "        self.fc4 = nn.Linear(256, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc5 = nn.Linear(128, 64)  # Hidden layer -> 64 neurons\n",
    "        self.fc6 = nn.Linear(64, 3)   # Output layer -> (theta_1, theta_2, theta_3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.leaky_relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Output angles in radians\n",
    "        return x\n",
    "\n",
    "model = InverseKinematicsNN()\n",
    "model.load_state_dict(torch.load('NN_50_000epoch_diffLATESTV2'))\n",
    "\n",
    "test_position = torch.tensor(fruit_position_base_system, dtype=torch.float32)/1000\n",
    "print('\\nTest pos (m): ', test_position)\n",
    "print()\n",
    "\n",
    "# Put your model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    predicted_angles = model(test_position)  # Predict the joint angles from the model\n",
    "    predicted_angles_rad = predicted_angles.detach().numpy()\n",
    "\n",
    "print('Predicted angles [rad]: ', predicted_angles_rad)\n",
    "predicted_angles_deg = predicted_angles_rad*180/np.pi\n",
    "print('Predicted angles [deg]: ', predicted_angles_deg)\n",
    "print()\n",
    "\n",
    "def angle_to_millisec(ls): #body, shoulder, elbow\n",
    "    theta1, theta2, theta3 = ls\n",
    "    \n",
    "    body_servo = 540 + ((-theta1+90)/180) * (2320 - 540) \n",
    "    shoulder_servo = 1250 + (theta2/90 * (2050 - 1250))\n",
    "    elbow_servo = 550 + ((theta3+90)/180 * (2370 - 550))\n",
    "    \n",
    "     # Clamp the servo positions to their respective limits\n",
    "    body_servo = max(540, min(body_servo, 2320))  # Min: 540, Max: 2320\n",
    "    shoulder_servo = max(1250, min(shoulder_servo, 2050))  # Min: 1250, Max: 2050\n",
    "    elbow_servo = max(550, min(elbow_servo, 2370))  # Min: 550, Max: 2370\n",
    "\n",
    "    return body_servo, shoulder_servo, elbow_servo\n",
    "\n",
    "body_servo, shoulder_servo, elbow_servo = angle_to_millisec(predicted_angles_deg)\n",
    "\n",
    "print('Body servo [ms]: ', body_servo)\n",
    "print('Shoulder servo [ms]: ', shoulder_servo)\n",
    "print('Elbow servo [ms]: ', elbow_servo)\n",
    "print()\n",
    "\n",
    "import serial\n",
    "import time\n",
    "\n",
    "def send_command(arduino, body, shoulder, elbow, gripper=None):\n",
    "    # Send the servo values to the Arduino as a single string\n",
    "    servo_data = f\"{body},{shoulder},{elbow}\"\n",
    "    if gripper is not None:\n",
    "        servo_data += f\",{gripper}\"  # Add gripper value if specified\n",
    "    servo_data += \"\\n\"\n",
    "    arduino.write(servo_data.encode('utf-8'))\n",
    "    time.sleep(1)  # Give some time for Arduino to process the command\n",
    "\n",
    "# Establish serial connection\n",
    "port_name = '/dev/cu.usbmodem101'\n",
    "arduino = serial.Serial(port=port_name, baudrate=57600, timeout=.2)\n",
    "arduino.dtr = False  # Prevent Arduino reset on connection\n",
    "time.sleep(2)  # Delay to stabilize serial connection\n",
    "\n",
    "# Send the initial servo positions to the Arduino\n",
    "send_command(arduino, body_servo, shoulder_servo, elbow_servo)\n",
    "\n",
    "# Wait for Arduino response (optional, can be removed if not needed)\n",
    "response = arduino.readline().decode('utf-8').strip()\n",
    "if response:\n",
    "    print(\"Arduino says:\", response)\n",
    "\n",
    "# Wait for manual input from the user\n",
    "while True:\n",
    "    command = input(\"Enter 'close' to close the gripper and move the robot: \")\n",
    "    if command.lower() == \"close\":\n",
    "\n",
    "        print('OK')\n",
    "        \n",
    "        command += \"\\n\"\n",
    "        arduino.write(command.encode('utf-8'))\n",
    "        time.sleep(5)\n",
    "        \n",
    "        break  # End the loop and finish the program\n",
    "    else:\n",
    "        print(\"Unknown command. Please type 'close' to continue.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
