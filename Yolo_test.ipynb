{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b754e4f3-1645-4d62-8d53-d636524ae941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77764d68-47ee-40cb-ab1a-7e81e6878da6",
   "metadata": {},
   "source": [
    "## Yolo model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a637150-5d5d-495d-8ac1-6a4c85c5f064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 bananas, 3 oranges, 5630.7ms\n",
      "Speed: 12.1ms preprocess, 5630.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained YOLOv8 model\n",
    "model = YOLO('yolov8l.pt')  # Load YOLOv8 large model\n",
    "model.to('cpu')  # Ensure the model is running on the CPU\n",
    "\n",
    "# Function to detect objects in an image\n",
    "def detect_objects(image_path):\n",
    "    # Check if the image file exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Error: The file '{image_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if img is None:\n",
    "        print(f\"Error: Failed to load the image from '{image_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Resize the image\n",
    "    img_resized = img #cv2.resize(img, (1280, 1280))\n",
    "\n",
    "    # Use the model to detect objects\n",
    "    results = model(img_resized)  # Pass the image to the YOLO model\n",
    "    #print(results[0])  # Print the results\n",
    "    results[0].save()  # Save the detection results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = detect_objects('plastic_fruits.jpeg')  # Replace with the path to your image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6deb47-c290-4b4b-bd11-fb55575fa7df",
   "metadata": {},
   "source": [
    "## A) Find the target fruit and mark all other fruits as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239d6900-9dc3-4586-a6cf-dd34d28589f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/johanlarsson/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\n",
      "0: 640x640 2 bananas, 3 oranges, 1767.8ms\n",
      "Speed: 3.9ms preprocess, 1767.8ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Image saved with marked fruits as: detected_fruits_with_positions.jpg\n",
      "Detected banana at positions: [(274, 269), (486, 521)]\n",
      "Banana detected. Robot can proceed to pick it up.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load YOLOv8 large model\n",
    "model = YOLO('yolov8l.pt')\n",
    "model.to('cpu')\n",
    "\n",
    "\n",
    "def detect_fruit_and_save_image(image_path, target_fruit='apple', output_path='output_image.jpg'):\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Error: The file '{image_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if img is None:\n",
    "        print(f\"Error: Failed to load the image from '{image_path}'.\")\n",
    "        return None\n",
    "\n",
    "    # Resize the image for YOLO detection (optional)\n",
    "    img_resized = img  # cv2.resize(img, (1280, 1280))\n",
    "\n",
    "    # Use the YOLO model to detect objects\n",
    "    results = model(img_resized)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = model.names\n",
    "\n",
    "    # Find the class ID for the target fruit\n",
    "    target_fruit_class_id = None\n",
    "    for class_id, name in class_names.items():\n",
    "        if name == target_fruit:\n",
    "            target_fruit_class_id = class_id\n",
    "            break\n",
    "\n",
    "    if target_fruit_class_id is None:\n",
    "        print(f\"Error: The target fruit '{target_fruit}' is not in the YOLO dataset.\")\n",
    "        return None\n",
    "\n",
    "    target_fruit_positions = []\n",
    "\n",
    "    # Loop over detections\n",
    "    for detection in results[0].boxes:\n",
    "        class_id = int(detection.cls)  # Class ID of the detected object\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = map(int, detection.xyxy[0])\n",
    "        # Calculate the center of the bounding box\n",
    "        x_center = (xmin + xmax) // 2\n",
    "        y_center = (ymin + ymax) // 2\n",
    "\n",
    "        # Draw a red dot if the detected fruit is the target fruit, else blue dot\n",
    "        if class_id == target_fruit_class_id:\n",
    "            cv2.circle(img_resized, (x_center, y_center), 20, (0, 0, 255), -1)  # Red dot for target fruit\n",
    "            target_fruit_positions.append((x_center, y_center))\n",
    "        elif class_id in [46, 47, 49, 50, 51]:  # IDs corresponding to fruits (banana, apple, orange, broccoli, carrot)\n",
    "            cv2.circle(img_resized, (x_center, y_center), 20, (255, 0, 0), -1)  # Blue dot for other fruits\n",
    "\n",
    "    # Save the detection image\n",
    "    cv2.imwrite(output_path, img_resized)\n",
    "    print(f\"Image saved with marked fruits as: {output_path}\")\n",
    "\n",
    "    if target_fruit_positions:\n",
    "        print(f\"Detected {target_fruit} at positions: {target_fruit_positions}\")\n",
    "        return target_fruit_positions\n",
    "    else:\n",
    "        print(f\"No {target_fruit} detected in the image.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = 'plastic_fruits.jpeg'  # Replace with image path\n",
    "output_image = 'detected_fruits_with_positions.jpg'  # Path to save the image with marked dots\n",
    "target_fruit = 'banana'  # Specify the fruit to mark with red dots\n",
    "fruit_positions = detect_fruit_and_save_image(image_path, target_fruit, output_image)\n",
    "\n",
    "if fruit_positions:\n",
    "    print(f\"{target_fruit.capitalize()} detected. Robot can proceed to pick it up.\")\n",
    "else:\n",
    "    print(f\"No {target_fruit} detected. Robot should not proceed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1575942-e0c4-42ba-9130-a2d52273f7f2",
   "metadata": {},
   "source": [
    "## B) Find coordinates from picture by clicking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2bfb882-4ac0-4792-a036-ebfeeb5ae994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 14:44:39.842 python[85706:2501850] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-02 14:44:39.842 python[85706:2501850] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates: (379, 167)\n",
      "Coordinates: (1297, 151)\n",
      "Coordinates: (1301, 708)\n",
      "Coordinates: (394, 724)\n",
      "Collected coordinates: [(379, 167), (1297, 151), (1301, 708), (394, 724)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Callback function to capture mouse click events\n",
    "def get_coordinates(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # Left mouse button click\n",
    "        print(f\"Coordinates: ({x}, {y})\")  # Print the coordinates\n",
    "        coordinates.append((x, y))  # Save the coordinates in a list\n",
    "\n",
    "# Load the image\n",
    "image_path = 'box.jpeg'  # Replace with your image path\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Create a list to store coordinates\n",
    "coordinates = []\n",
    "\n",
    "# Display the image in a window\n",
    "cv2.imshow('Image', img)\n",
    "cv2.setMouseCallback('Image', get_coordinates)  # Set the callback function\n",
    "\n",
    "# Wait until the user presses the ESC key\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print all collected coordinates\n",
    "print(\"Collected coordinates:\", coordinates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772a80e-f69e-4940-8cf0-2e8617a4db32",
   "metadata": {},
   "source": [
    "## Join A and B above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb09f9f-fe95-4a22-a36e-c259884c14b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates: (147, 7)\n",
      "Coordinates: (993, 5)\n",
      "Coordinates: (980, 711)\n",
      "Coordinates: (159, 703)\n",
      "\n",
      "0: 384x640 1 banana, 1 refrigerator, 978.3ms\n",
      "Speed: 1.8ms preprocess, 978.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Detected banana at positions: [(696, 487)]\n",
      "\n",
      "Pixels from banana to bottom-right corner:\n",
      "x (pixels): 284\n",
      "x (mm): 83.48049961472202\n",
      "\n",
      "y (pixles): 224\n",
      "y (mm): 65.8437743440061\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load YOLOv8 large model\n",
    "model = YOLO('yolov8l.pt')\n",
    "model.to('cpu')\n",
    "\n",
    "def detect_fruit_and_save_image(image_path, target_fruit='apple', output_path='output_image.jpg'):\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Error: The file '{image_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if img is None:\n",
    "        print(f\"Error: Failed to load the image from '{image_path}'.\")\n",
    "        return None\n",
    "\n",
    "    # Resize the image for YOLO detection (optional)\n",
    "    img_resized = img  # cv2.resize(img, (1280, 1280))\n",
    "\n",
    "    def get_coordinates(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:  # Left mouse button click\n",
    "            print(f\"Coordinates: ({x}, {y})\")  # Print the coordinates\n",
    "            coordinates_of_box.append((x, y))  # Save the coordinates in a list\n",
    "\n",
    "    coordinates_of_box = []\n",
    "\n",
    "    # MAKE SURE TO CLICK IN THIS ORDER: TOP-LEFT -> TOP-RIGHT -> BOTTOM-RIGHT -> BOTTOM-LEFT -> ANY KEY TO ESCAPE\n",
    "    cv2.imshow('Image', img_resized)\n",
    "    cv2.setMouseCallback('Image', get_coordinates)  # Set the callback function\n",
    "        \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Use the YOLO model to detect objects\n",
    "    results = model(img_resized)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = model.names\n",
    "\n",
    "    # Find the class ID for the target fruit\n",
    "    target_fruit_class_id = None\n",
    "    for class_id, name in class_names.items():\n",
    "        if name == target_fruit:\n",
    "            target_fruit_class_id = class_id\n",
    "            break\n",
    "\n",
    "    if target_fruit_class_id is None:\n",
    "        print(f\"Error: The target fruit '{target_fruit}' is not in the YOLO dataset.\")\n",
    "        return None\n",
    "\n",
    "    target_fruit_positions = []\n",
    "\n",
    "    # Loop over detections\n",
    "    for detection in results[0].boxes:\n",
    "        class_id = int(detection.cls)  # Class ID of the detected object\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = map(int, detection.xyxy[0])\n",
    "        # Calculate the center of the bounding box\n",
    "        x_center = (xmin + xmax) // 2\n",
    "        y_center = (ymin + ymax) // 2\n",
    "\n",
    "        # Draw a red dot if the detected fruit is the target fruit, else blue dot\n",
    "        if class_id == target_fruit_class_id:\n",
    "            cv2.circle(img_resized, (x_center, y_center), 10, (0, 0, 255), -1)  # Red dot for target fruit\n",
    "            target_fruit_positions.append((x_center, y_center))\n",
    "        elif class_id in [46, 47, 49, 50, 51]:  # IDs corresponding to fruits (banana, apple, orange, broccoli, carrot)\n",
    "            cv2.circle(img_resized, (x_center, y_center), 10, (255, 0, 0), -1)  # Blue dot for other fruits\n",
    "\n",
    "    for (x, y) in coordinates_of_box:\n",
    "        cv2.circle(img_resized, (x, y), 10, (0, 255, 0), -1)\n",
    "\n",
    "    \n",
    "    # Save the detection image\n",
    "    cv2.imwrite(output_path, img_resized)\n",
    "\n",
    "    if target_fruit_positions:\n",
    "        print(f\"Detected {target_fruit} at positions: {target_fruit_positions}\")\n",
    "    else:\n",
    "        print(f\"No {target_fruit} detected in the image.\")\n",
    "\n",
    "    x_zero, y_zero = coordinates_of_box[2] #bottom-right corner of box\n",
    "    x_fruit, y_fruit = target_fruit_positions[0]\n",
    "\n",
    "    box_size_x_mm = 300 #mm (measured IRL)\n",
    "    box_size_y_mm = 150 #mm (measured IRL)\n",
    "    box_size_diag_mm = (box_size_x_mm**2 + box_size_y_mm**2)**0.5 #mm (measured IRL)\n",
    "\n",
    "    # Calculate distances from picture in x\n",
    "    box_size_x_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[1])) #mm (width in pixels from picture)\n",
    "    box_size_x_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[2])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "\n",
    "    # Calculate distances from picture in y\n",
    "    box_size_y_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "    box_size_y_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[1])-np.array(coordinates_of_box[2])) #mm (width in pixels from picture)\n",
    "\n",
    "    # Calculate distances from picture in diagonals\n",
    "    box_size_diag_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[2])) #mm (width in pixels from picture)\n",
    "    box_size_diag_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[1])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "\n",
    "    # Get mean distances for x and y\n",
    "    box_size_x_pixels_mean = (box_size_x_pixels_1 + box_size_x_pixels_2) / 2\n",
    "    box_size_y_pixels_mean = (box_size_y_pixels_1 + box_size_y_pixels_2) / 2\n",
    "    box_size_diag_pixels_mean = (box_size_diag_pixels_1 + box_size_diag_pixels_2) / 2\n",
    "\n",
    "    # Get mm per pixel for x and y\n",
    "    mm_per_pixel_x = box_size_x_mm / box_size_x_pixels_mean # mm / pixel\n",
    "    mm_per_pixel_y = box_size_y_mm / box_size_y_pixels_mean # mm / pixel\n",
    "    mm_per_pixel_diag = box_size_diag_mm / box_size_diag_pixels_mean # mm / pixel\n",
    "\n",
    "    # Get overall mean mm per pixel\n",
    "    mm_per_pixel = (mm_per_pixel_x + mm_per_pixel_y + mm_per_pixel_diag) / 3\n",
    "    \n",
    "    relative_x_fruit_pixels = abs(x_fruit - x_zero)\n",
    "    relative_y_fruit_pixels = abs(y_fruit - y_zero)\n",
    "\n",
    "    relative_x_fruit_mm = relative_x_fruit_pixels*mm_per_pixel\n",
    "    relative_y_fruit_mm = relative_y_fruit_pixels*mm_per_pixel\n",
    "    \n",
    "    print(f'\\nPixels from {target_fruit} to bottom-right corner:')\n",
    "    print('x (pixels):', relative_x_fruit_pixels)\n",
    "    print('x (mm):', relative_x_fruit_mm)\n",
    "    print()\n",
    "    print('y (pixles):', relative_y_fruit_pixels)\n",
    "    print('y (mm):', relative_y_fruit_mm)\n",
    "\n",
    "    return relative_x_fruit_mm, relative_y_fruit_mm\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = 'ban.png'  # Replace with image path\n",
    "output_image = 'box_with_fruits_and_edges_marked.jpg'  # Path to save the image with marked dots\n",
    "target_fruit = 'banana'  # Specify the fruit to mark with red dots\n",
    "relative_x_fruit_mm, relative_y_fruit_mm = detect_fruit_and_save_image(image_path, target_fruit, output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5a435-cd6a-4183-b7f9-b23edf093d93",
   "metadata": {},
   "source": [
    "## Transform relative fruit position to base system coordinates in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec46989-9b00-4147-a64e-ec08fcf6323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90, 121.69313257248571, 121.69313257248571]\n"
     ]
    }
   ],
   "source": [
    "# Distances from bottom-right corner of box to the base system of Hubert\n",
    "box_bottom_right_corner_to_base_system_x = 20 #mm\n",
    "box_bottom_right_corner_to_base_system_y = 0  #mm\n",
    "box_bottom_right_corner_to_base_system_z = 50 #mm\n",
    "\n",
    "relative_z_fruit_mm = 40 #start with set value to begin with\n",
    "\n",
    "fruit_position_base_system_x = box_bottom_right_corner_to_base_system_x + relative_y_fruit_mm\n",
    "fruit_position_base_system_y = box_bottom_right_corner_to_base_system_y + relative_x_fruit_mm\n",
    "fruit_position_base_system_x = box_bottom_right_corner_to_base_system_z + relative_z_fruit_mm\n",
    "\n",
    "fruit_position_base_system = [fruit_position_base_system_x, fruit_position_base_system_y, fruit_position_base_system_y]\n",
    "\n",
    "print(fruit_position_base_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1b06c-c4c8-4c03-8cff-680d1ac2c335",
   "metadata": {},
   "source": [
    "## Forward kinematics for Hubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fc09e1-6b5f-4ca9-a2d3-b0e61b75815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End-effector position [x, y, z]: [    0.30032     0.15465     0.32899]\n"
     ]
    }
   ],
   "source": [
    "# Define rotation matrices and translation matrix from Part A\n",
    "def RotX(theta):\n",
    "    return np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, np.cos(theta), -np.sin(theta), 0],\n",
    "        [0, np.sin(theta), np.cos(theta), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "def RotY(theta):\n",
    "    return np.array([\n",
    "        [np.cos(theta), 0, np.sin(theta), 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-np.sin(theta), 0, np.cos(theta), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "def RotZ(theta):\n",
    "    return np.array([\n",
    "        [np.cos(theta), -np.sin(theta), 0, 0],\n",
    "        [np.sin(theta), np.cos(theta), 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "def translation_matrix(t_x, t_y, t_z):\n",
    "    return np.array([\n",
    "        [1, 0, 0, t_x],\n",
    "        [0, 1, 0, t_y],\n",
    "        [0, 0, 1, t_z],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "# Forward kinematics function\n",
    "def forward_kinematics(theta_1, theta_2, theta_3):\n",
    "    # Link lengths from Part A (in meters)\n",
    "    L1 = 0.055\n",
    "    L2 = 0.315 \n",
    "    L3 = 0.045 \n",
    "    L4 = 0.108 \n",
    "    L5 = 0.005 \n",
    "    L6 = 0.034 \n",
    "    L7 = 0.015 \n",
    "    L8 = 0.088 \n",
    "    L9 = 0.204 \n",
    "\n",
    "    # Transformations between coordinate frames\n",
    "    trans_0_to_1 = translation_matrix(L6, -L4, (L2 + L3))\n",
    "    trans_1_to_2 = translation_matrix(L7, -L8, -L5)\n",
    "    trans_2_to_3 = translation_matrix(0, -L9, 0)\n",
    "\n",
    "    # Rotations around the Z-axis for each joint\n",
    "    rot_theta_z_1 = RotZ(theta_1)\n",
    "    rot_theta_z_2 = RotZ(theta_2)\n",
    "    rot_theta_z_3 = RotZ(theta_3)\n",
    "\n",
    "    # Transformation from frame 0 to frame 1 (including rotation and translation)\n",
    "    trans_1 = rot_theta_z_1 @ trans_0_to_1 @ RotX(np.pi / 2)\n",
    "\n",
    "    # Transformation from frame 1 to frame 2\n",
    "    trans_2 = rot_theta_z_2 @ trans_1_to_2\n",
    "\n",
    "    # Transformation from frame 2 to frame 3\n",
    "    trans_3 = rot_theta_z_3 @ trans_2_to_3\n",
    "\n",
    "    # Homogeneous coordinates of the end-effector\n",
    "    end_effector_position = np.array([0, 0, 0, 1])\n",
    "\n",
    "    # Final position of the end-effector in the base frame\n",
    "    final_position = trans_1 @ trans_2 @ trans_3 @ end_effector_position\n",
    "\n",
    "    # Return the end-effector position (x, y, z) in the base frame\n",
    "    return final_position[:3]\n",
    "\n",
    "# Example usage\n",
    "theta_1 = np.pi / 4  # Example joint angles in radians\n",
    "theta_2 = np.pi / 3\n",
    "theta_3 = np.pi / 6\n",
    "\n",
    "position = forward_kinematics(theta_1, theta_2, theta_3)\n",
    "print(\"End-effector position [x, y, z]:\", position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c0c4a-e772-4118-8bc9-310e8656580c",
   "metadata": {},
   "source": [
    "## Use pretrained NN to find joint angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840bd1c8-3852-435b-9368-d1aef5ce7931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test pos:  tensor([0.0900, 0.1217, 0.1217])\n",
      "Predicted angles [rad]:  [     2.2952   -0.070567   -0.046909]\n",
      "Predicted angles [deg]:  [      131.5     -4.0432     -2.6877]\n"
     ]
    }
   ],
   "source": [
    "class InverseKinematicsNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InverseKinematicsNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)  # Input layer (x, y, z) -> 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc3 = nn.Linear(128, 256)  # Hidden layer -> 128 neurons\n",
    "        self.fc4 = nn.Linear(256, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc5 = nn.Linear(128, 64)  # Hidden layer -> 64 neurons\n",
    "        self.fc6 = nn.Linear(64, 3)   # Output layer -> (theta_1, theta_2, theta_3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.leaky_relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Output angles in radians\n",
    "        return x\n",
    "\n",
    "model = InverseKinematicsNN()\n",
    "model.load_state_dict(torch.load('NN_50_000epoch'))\n",
    "\n",
    "test_position = torch.tensor(fruit_position_base_system, dtype=torch.float32)/1000\n",
    "print('Test pos: ', test_position)\n",
    "\n",
    "# Put your model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    predicted_angles = model(test_position)  # Predict the joint angles from the model\n",
    "    predicted_angles = predicted_angles.detach().numpy()\n",
    "\n",
    "print('Predicted angles [rad]: ', predicted_angles)\n",
    "print('Predicted angles [deg]: ', predicted_angles*180/np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5d15e-6e03-4c04-a386-169ccfc9a347",
   "metadata": {},
   "source": [
    "## All shit togheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587e749f-aaca-4973-aa45-fb0fbbb2ef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 14:00:37.723 python[3171:57767] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-07 14:00:37.723 python[3171:57767] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates: (274, 22)\n",
      "Coordinates: (1267, 23)\n",
      "Coordinates: (1240, 850)\n",
      "Coordinates: (293, 837)\n",
      "\n",
      "0: 448x640 1 banana, 1 apple, 1 refrigerator, 4288.7ms\n",
      "Speed: 3.2ms preprocess, 4288.7ms inference, 5.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Detected banana at positions: [(582, 439)]\n",
      "\n",
      "Pixels from banana to bottom-right corner:\n",
      "x (pixels): 658\n",
      "x (mm): 191.7639379170847\n",
      "\n",
      "y (pixles): 411\n",
      "y (mm): 119.77960255915168\n",
      "Fruit pos base system:  [179.77960255915167, 191.7639379170847, 124]\n",
      "\n",
      "Test pos:  tensor([0.1798, 0.1918, 0.1240])\n",
      "\n",
      "Predicted angles [rad]:  [     1.9188     0.83187    0.055525]\n",
      "Predicted angles [deg]:  [     109.94      47.663      3.1813]\n",
      "\n",
      "Body servo [ms]:  1232.8462380303279\n",
      "Shoulder servo [ms]:  1889.5703676011826\n",
      "Elbow servo [ms]:  1630.5584633350372\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load YOLOv8 large model\n",
    "model = YOLO('yolov8l.pt')\n",
    "model.to('cpu')\n",
    "\n",
    "def detect_fruit_and_save_image(image_path, target_fruit='apple', output_path='output_image.jpg'):\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Error: The file '{image_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if img is None:\n",
    "        print(f\"Error: Failed to load the image from '{image_path}'.\")\n",
    "        return None\n",
    "\n",
    "    # Resize the image for YOLO detection (optional)\n",
    "    img_resized = cv2.resize(img, (1300, 900))\n",
    "\n",
    "    #img_resized = cv2.rotate(img_resized, cv2.ROTATE_180)\n",
    "\n",
    "    def get_coordinates(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:  # Left mouse button click\n",
    "            print(f\"Coordinates: ({x}, {y})\")  # Print the coordinates\n",
    "            coordinates_of_box.append((x, y))  # Save the coordinates in a list\n",
    "\n",
    "    coordinates_of_box = []\n",
    "\n",
    "    # MAKE SURE TO CLICK IN THIS ORDER: TOP-LEFT -> TOP-RIGHT -> BOTTOM-RIGHT -> BOTTOM-LEFT -> ANY KEY TO ESCAPE\n",
    "    cv2.imshow('Image', img_resized)\n",
    "    cv2.setMouseCallback('Image', get_coordinates)  # Set the callback function\n",
    "        \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Use the YOLO model to detect objects\n",
    "    results = model(img_resized)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = model.names\n",
    "\n",
    "    # Find the class ID for the target fruit\n",
    "    target_fruit_class_id = None\n",
    "    for class_id, name in class_names.items():\n",
    "        if name == target_fruit:\n",
    "            target_fruit_class_id = class_id\n",
    "            break\n",
    "\n",
    "    if target_fruit_class_id is None:\n",
    "        print(f\"Error: The target fruit '{target_fruit}' is not in the YOLO dataset.\")\n",
    "        return None\n",
    "\n",
    "    target_fruit_positions = []\n",
    "\n",
    "    # Loop over detections\n",
    "    for detection in results[0].boxes:\n",
    "        class_id = int(detection.cls)  # Class ID of the detected object\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = map(int, detection.xyxy[0])\n",
    "        # Calculate the center of the bounding box\n",
    "        x_center = (xmin + xmax) // 2\n",
    "        y_center = (ymin + ymax) // 2\n",
    "\n",
    "        # Draw a red dot if the detected fruit is the target fruit, else blue dot\n",
    "        if class_id == target_fruit_class_id:\n",
    "            cv2.circle(img_resized, (x_center, y_center), 10, (0, 0, 255), -1)  # Red dot for target fruit\n",
    "            target_fruit_positions.append((x_center, y_center))\n",
    "        elif class_id in [46, 47, 49, 50, 51]:  # IDs corresponding to fruits (banana, apple, orange, broccoli, carrot)\n",
    "            cv2.circle(img_resized, (x_center, y_center), 10, (255, 0, 0), -1)  # Blue dot for other fruits\n",
    "\n",
    "    for (x, y) in coordinates_of_box:\n",
    "        cv2.circle(img_resized, (x, y), 10, (0, 255, 0), -1)\n",
    "\n",
    "    \n",
    "    # Save the detection image\n",
    "    cv2.imwrite(output_path, img_resized)\n",
    "\n",
    "    if target_fruit_positions:\n",
    "        print(f\"Detected {target_fruit} at positions: {target_fruit_positions}\")\n",
    "    else:\n",
    "        print(f\"No {target_fruit} detected in the image.\")\n",
    "\n",
    "    x_zero, y_zero = coordinates_of_box[2] #bottom-right corner of box\n",
    "    x_fruit, y_fruit = target_fruit_positions[0]\n",
    "\n",
    "    box_size_x_mm = 282 #mm (measured IRL)\n",
    "    box_size_y_mm = 240 #mm (measured IRL)\n",
    "    box_size_diag_mm = (box_size_x_mm**2 + box_size_y_mm**2)**0.5 #mm (measured IRL)\n",
    "\n",
    "    # Calculate distances from picture in x\n",
    "    box_size_x_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[1])) #mm (width in pixels from picture)\n",
    "    box_size_x_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[2])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "\n",
    "    # Calculate distances from picture in y\n",
    "    box_size_y_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "    box_size_y_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[1])-np.array(coordinates_of_box[2])) #mm (width in pixels from picture)\n",
    "\n",
    "    # Calculate distances from picture in diagonals\n",
    "    box_size_diag_pixels_1 = np.linalg.norm(np.array(coordinates_of_box[0])-np.array(coordinates_of_box[2])) #mm (width in pixels from picture)\n",
    "    box_size_diag_pixels_2 = np.linalg.norm(np.array(coordinates_of_box[1])-np.array(coordinates_of_box[3])) #mm (width in pixels from picture)\n",
    "\n",
    "    # Get mean distances for x and y\n",
    "    box_size_x_pixels_mean = (box_size_x_pixels_1 + box_size_x_pixels_2) / 2\n",
    "    box_size_y_pixels_mean = (box_size_y_pixels_1 + box_size_y_pixels_2) / 2\n",
    "    box_size_diag_pixels_mean = (box_size_diag_pixels_1 + box_size_diag_pixels_2) / 2\n",
    "\n",
    "    # Get mm per pixel for x and y\n",
    "    mm_per_pixel_x = box_size_x_mm / box_size_x_pixels_mean # mm / pixel\n",
    "    mm_per_pixel_y = box_size_y_mm / box_size_y_pixels_mean # mm / pixel\n",
    "    mm_per_pixel_diag = box_size_diag_mm / box_size_diag_pixels_mean # mm / pixel\n",
    "\n",
    "    # Get overall mean mm per pixel\n",
    "    mm_per_pixel = (mm_per_pixel_x + mm_per_pixel_y + mm_per_pixel_diag) / 3\n",
    "    \n",
    "    relative_x_fruit_pixels = abs(x_fruit - x_zero)\n",
    "    relative_y_fruit_pixels = abs(y_fruit - y_zero)\n",
    "\n",
    "    relative_x_fruit_mm = relative_x_fruit_pixels*mm_per_pixel\n",
    "    relative_y_fruit_mm = relative_y_fruit_pixels*mm_per_pixel\n",
    "    \n",
    "    print(f'\\nPixels from {target_fruit} to bottom-right corner:')\n",
    "    print('x (pixels):', relative_x_fruit_pixels)\n",
    "    print('x (mm):', relative_x_fruit_mm)\n",
    "    print()\n",
    "    print('y (pixles):', relative_y_fruit_pixels)\n",
    "    print('y (mm):', relative_y_fruit_mm)\n",
    "\n",
    "    return relative_x_fruit_mm, relative_y_fruit_mm\n",
    "\n",
    "# \n",
    "image_path = 'ban.jpg'  # Replace with image path\n",
    "output_image = 'box_with_fruits_and_edges_marked.jpg'  # Path to save the image with marked dots\n",
    "target_fruit = 'banana'  # Specify the fruit to mark with red dots\n",
    "relative_x_fruit_mm, relative_y_fruit_mm = detect_fruit_and_save_image(image_path, target_fruit, output_image)\n",
    "\n",
    "# Distances from bottom-right corner of box to the base system of Hubert\n",
    "box_bottom_right_corner_to_base_system_x = 60 #mm\n",
    "box_bottom_right_corner_to_base_system_y = 0  #mm\n",
    "box_bottom_right_corner_to_base_system_z = 94 #mm\n",
    "\n",
    "relative_z_fruit_mm = 30 #start with set value to begin with\n",
    "\n",
    "fruit_position_base_system_x = box_bottom_right_corner_to_base_system_x + relative_y_fruit_mm\n",
    "fruit_position_base_system_y = box_bottom_right_corner_to_base_system_y + relative_x_fruit_mm\n",
    "fruit_position_base_system_z = box_bottom_right_corner_to_base_system_z + relative_z_fruit_mm\n",
    "\n",
    "fruit_position_base_system = [fruit_position_base_system_x, fruit_position_base_system_y, fruit_position_base_system_z]\n",
    "\n",
    "print(\"Fruit pos base system: \", fruit_position_base_system)\n",
    "\n",
    "class InverseKinematicsNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InverseKinematicsNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)  # Input layer (x, y, z) -> 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc3 = nn.Linear(128, 256)  # Hidden layer -> 128 neurons\n",
    "        self.fc4 = nn.Linear(256, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc5 = nn.Linear(128, 64)  # Hidden layer -> 64 neurons\n",
    "        self.fc6 = nn.Linear(64, 3)   # Output layer -> (theta_1, theta_2, theta_3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.leaky_relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Output angles in radians\n",
    "        return x\n",
    "\n",
    "model = InverseKinematicsNN()\n",
    "model.load_state_dict(torch.load('NN_50_000epoch'))\n",
    "\n",
    "test_position = torch.tensor(fruit_position_base_system, dtype=torch.float32)/1000\n",
    "print()\n",
    "print('Test pos: ', test_position)\n",
    "print()\n",
    "\n",
    "# Put your model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    predicted_angles = model(test_position)  # Predict the joint angles from the model\n",
    "    predicted_angles_rad = predicted_angles.detach().numpy()\n",
    "\n",
    "print('Predicted angles [rad]: ', predicted_angles_rad)\n",
    "predicted_angles_deg = predicted_angles_rad*180/np.pi\n",
    "print('Predicted angles [deg]: ', predicted_angles_deg)\n",
    "print()\n",
    "\n",
    "def angle_to_millisec(ls): #body, shoulder, elbow\n",
    "    theta1, theta2, theta3 = ls\n",
    "    \n",
    "    body_servo = 2320 - (theta1/180 * (2320 - 540)) #theta1/180 * (2330 - 560) + 1700 #560\n",
    "    shoulder_servo = 2300 - (theta2/180 * (2300 - 750))\n",
    "    elbow_servo = 1650 - (theta3/180 * (1650 - 550))\n",
    "    \n",
    "    return body_servo,shoulder_servo, elbow_servo\n",
    "\n",
    "body_servo, shoulder_servo, elbow_servo = angle_to_millisec(predicted_angles_deg)\n",
    "\n",
    "print('Body servo [ms]: ', body_servo)\n",
    "print('Shoulder servo [ms]: ', shoulder_servo)\n",
    "print('Elbow servo [ms]: ', elbow_servo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b8f892e-a13f-414d-b8d4-1d5ee9558976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body servo [ms]:  1276.6265563964844\n",
      "Shoulder servo [ms]:  2297.9920609792075\n",
      "Elbow servo [ms]:  604.8273184564379\n"
     ]
    }
   ],
   "source": [
    "def angle_to_millisec(ls): #body, shoulder, elbow\n",
    "    theta1, theta2, theta3 = ls\n",
    "    \n",
    "    body_servo = 2320 - (theta1/180 * (2320 - 540)) \n",
    "    shoulder_servo = 2300 - (theta2/180 * (2300 - 1100))\n",
    "    elbow_servo = 550 + (theta3/90 * (2400 - 550)) # 1650 - (theta3/90 * (2400 - 550))\n",
    "    \n",
    "    return body_servo,shoulder_servo, elbow_servo\n",
    "\n",
    "body_servo, shoulder_servo, elbow_servo = angle_to_millisec(predicted_angles_deg)\n",
    "\n",
    "print('Body servo [ms]: ', body_servo)\n",
    "print('Shoulder servo [ms]: ', shoulder_servo)\n",
    "print('Elbow servo [ms]: ', elbow_servo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c346427-cdfd-4cf0-91aa-89bd607daea3",
   "metadata": {},
   "source": [
    "# OLD SHIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db01672-3eb9-431f-9fee-fa6db9ba997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 0.2258\n",
      "Epoch [200/2000], Loss: 0.1775\n",
      "Epoch [300/2000], Loss: 0.1446\n",
      "Epoch [400/2000], Loss: 0.1104\n",
      "Epoch [500/2000], Loss: 0.0837\n",
      "Epoch [600/2000], Loss: 0.0708\n",
      "Epoch [700/2000], Loss: 0.0623\n",
      "Epoch [800/2000], Loss: 0.0579\n",
      "Epoch [900/2000], Loss: 0.0546\n",
      "Epoch [1000/2000], Loss: 0.0523\n",
      "Epoch [1100/2000], Loss: 0.0504\n",
      "Epoch [1200/2000], Loss: 0.0487\n",
      "Epoch [1300/2000], Loss: 0.0471\n",
      "Epoch [1400/2000], Loss: 0.0455\n",
      "Epoch [1500/2000], Loss: 0.0435\n",
      "Epoch [1600/2000], Loss: 0.0409\n",
      "Epoch [1700/2000], Loss: 0.0390\n",
      "Epoch [1800/2000], Loss: 0.0375\n",
      "Epoch [1900/2000], Loss: 0.0363\n",
      "Epoch [2000/2000], Loss: 0.0354\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network model\n",
    "class InverseKinematicsNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InverseKinematicsNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)  # Input layer (x, y, z) -> 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 128)  # Hidden layer -> 128 neurons\n",
    "        self.fc3 = nn.Linear(128, 64)  # Hidden layer -> 64 neurons\n",
    "        self.fc4 = nn.Linear(64, 3)   # Output layer -> (theta_1, theta_2, theta_3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # Output angles in radians\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = InverseKinematicsNN()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Generate synthetic training data (example)\n",
    "def generate_training_data(num_samples):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Random joint angles in radians\n",
    "        theta_1 = np.random.uniform(0, np.pi)\n",
    "        theta_2 = np.random.uniform(0, np.pi)\n",
    "        theta_3 = np.random.uniform(0, np.pi/2)\n",
    "\n",
    "        # Forward kinematics to get the end-effector position\n",
    "        pos = forward_kinematics(theta_1, theta_2, theta_3)\n",
    "\n",
    "        # Input: (x, y, z), Output: (theta_1, theta_2, theta_3)\n",
    "        X.append(pos)\n",
    "        Y.append([theta_1, theta_2, theta_3])\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
    "\n",
    "# Prepare data\n",
    "X_train, Y_train = generate_training_data(2000) #X=pos, Y=angels\n",
    "X_train = torch.tensor(X_train)\n",
    "Y_train = torch.tensor(Y_train)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27febc0e-3ad4-49c8-b4d8-8b717059c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted joint angles (theta_1, theta_2, theta_3): [    0.84689      1.1453      0.4227]\n",
      "True joint angles (theta_1, theta_2, theta_3): [0.7853981633974483, 1.0471975511965976, 0.5235987755982988]\n",
      "\n",
      "P_predicted  [    0.29199     0.17479     0.33678]\n",
      "P_target  [0.30031549, 0.15465149, 0.32899038]\n",
      "Error:  0.023141538501017588\n"
     ]
    }
   ],
   "source": [
    "# Test the model with an example input\n",
    "position = forward_kinematics(0, np.pi/2, 0)\n",
    "\n",
    "P_target = [0.30031549, 0.15465149, 0.32899038]\n",
    "\n",
    "model.eval()\n",
    "test_position = torch.tensor(P_target, dtype=torch.float32)  # Example (x, y, z)\n",
    "predicted_angles = model(test_position)\n",
    "predicted_angles = predicted_angles.detach().numpy()\n",
    "predicted_theta_1 = predicted_angles[0]\n",
    "predicted_theta_2 = predicted_angles[1]\n",
    "predicted_theta_3 = predicted_angles[2]\n",
    "\n",
    "P_predicted = forward_kinematics(predicted_theta_1, predicted_theta_2, predicted_theta_3)\n",
    "error = np.linalg.norm(P_target - P_predicted)\n",
    "\n",
    "theta_1 = np.pi / 4  # Example joint angles in radians\n",
    "theta_2 = np.pi / 3\n",
    "theta_3 = np.pi / 6\n",
    "\n",
    "print(\"Predicted joint angles (theta_1, theta_2, theta_3):\", predicted_angles)\n",
    "print(\"True joint angles (theta_1, theta_2, theta_3):\", [theta_1, theta_2, theta_3])\n",
    "print()\n",
    "print('P_predicted ', P_predicted)\n",
    "print('P_target ', P_target)\n",
    "print('Error: ', error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
